\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{xepersian}
\settextfont[Scale=1.2]{XB Zar}

% section numbering
\setcounter{secnumdepth}{3}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}

\title{ 
\begin{normalsize} به نام خدا \end{normalsize}
\\[2cm]
 گزارش پروژه فرایند دریکله
}
\author{علیرضا نوریان
\\
\\ \small دانشگاه علم و صنعت ایران
\\ \small nourian@comp.iust.ac.ir
}

\begin{document}
\maketitle

\section{مقدمه}
اولین مشکلی که معمولا برای خوشه‌بندی داده‌ها با آن روبرو هستیم، تعیین پارامترهای الگوریتم خوشه‌بندی است. برای حل این مشکل فرایندهایی مثل فرایند دریکله پیشنهاد شده‌اند که سعی می‌کنند داده‌ها را به بهترین حالت در مناسب‌ترین تعداد خوشه قرار دهند. فرایند دریکله از این منظر جزء الگوریتم‌های خوشه‌بندی بدون پارامتر است. البته راه حلهای دیگری نیز برای خوشه‌بندی بدون پارامتر پیشنهاد شده‌اند که در بسیاری از آنها تعداد خوشه‌ها نیز به عنوان یکی از شرایط مساله باید بهینه‌سازی شود و مثلا آزمایش مقادیر مختلف برای تعداد خوشه‌ها ما را به تعداد صحیح برساند.

فرایند دریکله با منطق دیگری به این مساله نزدیک می‌شود و اصلا هدف استفاده از نوعی ابتکار یا جستجو برای پیدا کردن تعداد صحیح خوشه‌ها نیست. بلکه فرایندی با پشتوانه نظری است که در آن تعداد و محتوی خوشه‌ها، در هر مرحله بهتر می‌شوند و ضامن موفقیت این فرایند، میل اصیل  داده‌ها برای قرارگیری در خوشه‌ای است که در واقع به آن تعلق دارند. البته برای دستیابی به این توصیف شیرین باید فرض کنیم داده‌ها از تعداد نامشخصی توزیع گوسی تولید شده‌اند که به طور عام فرض خوبی در مورد همه‌ی مجموعه‌های داده واقعی است.

\section{مبانی نظری}
فرض می‌کنیم تمام داده‌های ما از توزیع گوسی $G_0$ نمونه‌برداری شده‌اند. به این موضع به میزان $A_0$ اعتقاد داریم. در واقع $A_0$ بسیار کوچک فرض می‌شود که به معنی حدس ما در مورد داده‌ها است. به این ترتیب فرض می‌کنیم که همه‌ی داده‌ها در یک خوشه و با برچسب میانگین‌شان قرار داده شده‌اند. البته باید توجه داشته باشیم که برچسب تمام خوشه‌ها برای همگرا شدن پاسخ، به صورت صحیح در نظر گرفته می‌شود.

پس از در نظر گرفتن داده‌ها در یک خوشه با تکرار یک فرایند منطقی سعی می‌کنیم خوشه‌بندی داده‌ها را اصلاح کنیم. برای این منظور در هر تکرار برای هر نمونه مجددا تصمیم می‌گیریم که بهتر است در خوشه‌ی دیگری قرار بگیرد و یا حتی باید خوشه‌ی جدیدی برای آن ایجاد کنیم. در این فرایند مرتبا خوشه‌های احتمالا بی‌اهمیت تولید و حذف می‌شوند ولی در نهایت خوشه‌هایی پایدار می‌مانند که برچسب آنها به مراکز خوشه‌ی واقعی نزدیکتر باشد.

سرنوشت هر نمونه در هر دور تکرار فرایند از رابطه \ref{eq:main} مشخص می‌شود \cite{dirichlet-process}. در این رابطه $X_i$ برچسب کنونی نمونه است که با برچسب $X_j$ تعویض می‌شود و یا در خوشه‌ی جدیدی قرار می‌گیرد.  برای محاسبه برچسب خوشه جدید از $h(X_i \mid Y_j)$ نمونه‌گیری می‌کنیم.

\begin{equation} \label{eq:main}
{X}_{i} \mid {X}_{j}, i\neq j, Y 
\begin{cases} = {X}_{j} \quad probabilty \quad \frac{\phi ({Y}_{i}-{X}_{j})}{A({Y}_{i})+\sum _{l=1, l\neq i}^{n}{\phi ({Y}_{i}-{X}_{l})}} \\ \sim  h({X}_{i } \mid {Y}_{j}) \quad probabilty \quad \frac {A({Y}_{i})}{A({Y}_{i})+\sum _{l=1, l\neq i}^{n}{\phi ({Y}_{i}-{X}_{l})}}  \end{cases} 
\end{equation}
$$ A(Y) = {A}_{0} \int \phi(Y - X) {G}_{0}(dX) $$
$$ h(X_i \mid Y_i) = [A_0 / A(Y_i)] \phi(Y_i - X) g_0(X) $$

در رابطه \ref{eq:main}، $\phi$ نمایانگر مقدار تابع توزیع نرمال است و $A(Y_i)$ احتمال ایجاد خوشه‌ی جدید است. همانطور که در این رابطه مشخص است، $A(Y_i)$ از کانولوشن $\phi$ و $G_0$ بدست می‌آید. برای محاسبه‌ی مقدار این تابع در پیاده‌سازی از رابطه‌ی \ref{eq:newprob} استفاده شد که برای محاسبه کانولوشن دو توزیع گوسی است \cite{gaussian-distributions}.

\begin{equation} \label{eq:newprob}
A({Y}_{i})=\frac {1}{\sqrt {2\pi ({\sigma}_{f}^{2}+{\sigma}_{g}^{2})}} {e}^{ -\frac {{(x-({\mu}_{f}+{\mu}_{g}))}^{2}}{2({\sigma}_{f}^{2}+{\sigma}_{g}^{2})}}
\end{equation}

همچنین نمونه‌گیری از توزیع $h(X_i \mid Y_j)$ نیز با در نظر گرفتن گوسی بودن دو توزیع، به نمونه‌گیری از توزیعی با میانگین و انحراف معیار مشخص‌شده در رابطه \ref{eq:newlabel} ساده می‌شود \cite{gaussian-distributions}.

\begin{equation} \label{eq:newlabel}
{\mu}_{fg}=\frac {{\mu}_{f}{\sigma}_{g}^{2}+{\mu}_{g}{\sigma}_{f}^{2}}{{\sigma}_{f}^{2}+{\sigma}_{g}^{2}} \quad and\quad {\sigma}_{fg}=\sqrt {\frac {{\sigma}_{f}^{2}{\sigma}_{g}^{2}}{{\sigma}_{f}^{2}+{\sigma}_{g}^{2}}} 
\end{equation}

\section{جزئیات پیاده‌سازی}
با توجه به اینکه دانش پیشینی در مورد داده‌های مساله نداریم، بهترین تقریب ما از $G_0$ توزیع گوسی با میانگین و واریانس داده‌ها است. مجموعه پاسخ تولید شده به ازای $A_0 = 0.001$ به نظر منطقی می‌رسد اگرچه شاید همین‌قدر هم از به حدس خود در مورد توزیع اولیه داده‌ها مطمئن نباشیم.

در بخش قبل صحبت از ایجاد و حذف خوشه‌ها با سرعت بالا در این فرایند کردیم. انتظار ما از روند واقعی اجرای چنین فرایندی، بالا بودن نرخ ایجاد و حذف خوشه‌ها در آغاز فرایند و پایین بودن آن پس از چندین مرحله اجراست؛ چراکه در آغاز ما هیچ تصوری از تعداد و مرکز خوشه‌ها نداریم و باید با آزمودن مراکز مختلف به دنبال مراکز واقعی بگردیم و در مقابل وقتی به انتهای فرایند نزدیک می‌شویم توقع داریم که همه‌ی خوشه‌ها ایجاد شده باشند و فقط تغییرات بسیار کوچک در بدنه‌ی آنها را تحمل می‌کنیم.

برای اجرای چنین منطقی در فرایند دریکله از الگوریتم شبیه‌سازی تبرید فلزات\LTRfootnote{Simulated Annealing} استفاده کردیم. به این ترتیب دما در این الگوریتم نمایانگر احتمال ایجاد خوشه‌ی جدید است که با جلو رفتن مراحل فرایند، کاهش یافته و پاسخها به ثبات می‌رسند.

\section{نتایج}
برای آزمودن پیاده‌سازی، نتایج اجرای فرایند دریکله را روی دادگان یک بعدی مربوط به قد دانشجویان اجرا کردیم. این داده‌ها از به صورت مجازی از 10 خوشه (هر خوشه 90 عضو) تولید شده‌اند. در ادامه نتایج خوشه‌بندی پس از ۱۰۰ بار تکرار فرایند دریکله را مشاهده می‌کنیم که به نظر می‌رسد با خوشه‌های واقعی فاصله‌ی چندانی ندارد:
\\

\begin{latin}
Cluster1: label=150, mean=149.92, std=1.05

[151, 151, 151, 151, 151, 151, 151, 151, 150, 151, 150, 151, 150, 151, 150, 150, 152, 150, 150, 152, 150, 150, 151, 150, 151, 150, 150, 150, 150, 151, 152, 150, 150, 151, 149, 151, 150, 150, 149, 149, 151, 151, 149, 150, 152, 150, 150, 150, 150, 150, 150, 150, 149, 149, 149, 149, 152, 150, 150, 150, 150, 150, 149, 149, 149, 149, 149, 150, 151, 149, 148, 149, 151, 150, 148, 149, 148, 149, 149, 149, 150, 151, 148, 150, 149, 148, 147, 148, 148, 149]
\\

Cluster2: label=196, mean=196.04, std=1.03

[198, 197, 197, 197, 198, 197, 196, 196, 196, 196, 197, 198, 196, 196, 197, 197, 196, 197, 197, 196, 197, 196, 197, 197, 196, 196, 196, 197, 197, 195, 196, 198, 196, 195, 195, 196, 195, 197, 195, 195, 197, 197, 196, 199, 195, 196, 195, 196, 196, 197, 198, 196, 195, 195, 197, 196, 196, 196, 196, 197, 194, 194, 196, 196, 196, 196, 195, 195, 196, 195, 196, 195, 196, 195, 195, 194, 194, 196, 195, 197, 194, 194, 196, 197, 194, 196, 196, 196, 196, 196]
\\

Cluster3: label=184, mean=183.26, std=0.88

[184, 184, 184, 184, 184, 184, 184, 184, 184, 185, 184, 185, 184, 184, 184, 184, 184, 184, 185, 184, 184, 184, 184, 184, 185, 184, 184, 184, 184, 184, 184, 184, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 184, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 184, 183, 182, 182, 182, 182, 182, 182, 182, 183, 182, 182, 182, 182, 183, 182, 182, 182, 182, 182, 182, 182, 186]
\\

Cluster4: label=179, mean=178.91, std=1.06

[179, 179, 179, 179, 179, 179, 179, 179, 179, 179, 178, 179, 178, 179, 178, 179, 178, 179, 178, 179, 178, 179, 179, 179, 179, 179, 180, 179, 180, 179, 180, 180, 179, 179, 179, 180, 178, 180, 179, 180, 179, 180, 178, 178, 178, 178, 180, 179, 178, 178, 178, 178, 180, 178, 178, 178, 179, 178, 178, 178, 179, 180, 178, 180, 180, 180, 180, 180, 180, 180, 178, 180, 180, 181, 181, 181, 181, 178, 181, 181, 177, 177, 177, 177, 177, 177, 177, 177]
\\

Cluster5: label=158, mean=157.97, std=1.05

[157, 158, 157, 158, 157, 157, 158, 158, 157, 157, 157, 158, 159, 159, 158, 158, 158, 159, 159, 157, 159, 158, 158, 157, 157, 157, 159, 158, 157, 157, 157, 158, 159, 157, 159, 156, 157, 158, 158, 159, 157, 156, 159, 158, 159, 157, 158, 158, 158, 159, 156, 157, 158, 158, 158, 158, 159, 158, 158, 158, 157, 158, 159, 156, 159, 157, 158, 157, 157, 158, 159, 159, 158, 158, 159, 156, 159, 157, 157, 155, 159, 159, 158, 159, 160, 159, 160, 160, 160, 160, 160]
\\

Cluster6: label=175, mean=174.95, std=1.04

[175, 175, 175, 176, 176, 175, 176, 175, 175, 175, 176, 175, 176, 176, 175, 175, 175, 176, 176, 175, 175, 176, 176, 176, 176, 176, 175, 175, 175, 175, 176, 175, 175, 175, 175, 175, 175, 176, 175, 174, 176, 174, 174, 174, 175, 174, 175, 174, 174, 174, 175, 175, 174, 174, 174, 174, 174, 175, 174, 174, 175, 175, 176, 176, 174, 175, 175, 175, 174, 174, 174, 174, 174, 174, 176, 174, 174, 173, 173, 177, 177, 177, 177, 173, 173, 173, 173, 177, 177, 177, 173]
\\

Cluster7: label=167, mean=167.04, std=0.92

[167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 166, 167, 167, 166, 166, 166, 167, 166, 166, 168, 166, 168, 168, 168, 167, 168, 168, 166, 166, 166, 168, 166, 168, 167, 168, 166, 167, 168, 166, 167, 168, 166, 167, 168, 167, 167, 166, 167, 166, 166, 168, 168, 168, 166, 168, 168, 167, 168, 168, 166, 167, 168, 168, 168, 168, 166, 168, 168, 166, 166, 165, 169, 169, 165, 165, 169]
\\

Cluster8: label=188, mean=188.04, std=0.98

[189, 189, 189, 189, 189, 188, 189, 188, 189, 188, 188, 188, 189, 188, 188, 188, 188, 187, 187, 188, 189, 187, 187, 189, 188, 188, 188, 189, 189, 187, 187, 190, 189, 188, 187, 188, 188, 189, 189, 189, 187, 188, 189, 190, 189, 189, 188, 188, 189, 187, 187, 188, 189, 189, 187, 187, 187, 189, 188, 190, 187, 189, 188, 187, 187, 187, 188, 187, 187, 190, 188, 188, 188, 187, 190, 188, 187, 189, 189, 188, 187, 187, 187, 189, 187, 187, 187, 186, 186, 186]
\\

Cluster9: label=171, mean=171.05, std=1.10

[171, 171, 171, 171, 171, 171, 171, 171, 171, 172, 171, 172, 172, 172, 172, 171, 172, 171, 172, 171, 171, 172, 172, 172, 172, 172, 172, 172, 172, 171, 171, 171, 171, 172, 171, 171, 171, 171, 171, 172, 171, 171, 171, 170, 170, 171, 170, 170, 172, 172, 170, 172, 171, 171, 170, 170, 170, 170, 172, 170, 170, 171, 171, 170, 170, 170, 170, 171, 171, 171, 171, 170, 170, 170, 170, 170, 170, 172, 172, 172, 170, 172, 169, 173, 169, 173, 173, 173, 169, 169, 173, 173, 173, 169, 169, 173, 169, 173, 169, 173, 169]
\\

Cluster10: label=163, mean=163.18, std=1.06

[163, 163, 163, 163, 162, 163, 162, 162, 162, 163, 163, 162, 163, 163, 162, 162, 162, 163, 163, 163, 163, 163, 162, 163, 163, 163, 162, 163, 164, 163, 164, 164, 164, 163, 164, 163, 162, 163, 164, 163, 164, 163, 163, 163, 162, 164, 163, 164, 162, 164, 163, 161, 162, 164, 164, 163, 164, 163, 162, 163, 164, 164, 164, 162, 164, 164, 164, 164, 162, 161, 163, 164, 162, 164, 161, 164, 161, 164, 161, 165, 165, 165, 165, 165, 165, 165, 165, 165, 165]
\end{latin}

\renewcommand*{\refname}{\section{منابع}}
\begin{thebibliography}{9}
\begin{latin}

\bibitem{dirichlet-process}
M. D. Escobar, “Estimating normal means with a Dirichlet process prior,” Journal of the American Statistical Association, pp. 268–277, 1994.

\bibitem{gaussian-distributions}
P. Bromiley, “Products and Convolutions of Gaussian Distributions.”

\end{latin}
\end{thebibliography}

\end{document}
